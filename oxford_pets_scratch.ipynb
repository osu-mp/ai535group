{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ecaaaa9-56db-4bb1-a754-5e3c7c058303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models.detection as det\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee337ab9-cfee-47fd-bb34-6c701c27e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36cd25-9e5f-4d7f-a016-6bcc9f68eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, root_dir, xforms, yforms):\n",
    "        self.ann_dir = path.join(root_dir, \"annotations\", \"trimaps\")\n",
    "        self.image_dir = path.join(root_dir, \"images\")\n",
    "        self.image_files = glob.glob(path.join(self.image_dir, \"*\"))\n",
    "        self.image_files = [x for x in self.image_files if path.splitext(x)[1] == \".jpg\"]\n",
    "        self.image_files = [x for x in self.image_files if Image.open(x).format == \"JPEG\"]\n",
    "        self.image_files = [x for x in self.image_files if Image.open(x).mode == \"RGB\"]\n",
    "        self.last_mrcnn_idx = 0\n",
    "        self.breed_assoc = {x: self.last_mrcnn_idx + idx for \n",
    "                            idx, x in enumerate(sorted(list(set(\n",
    "                                [ path.basename('_'.join(fname.split(\"_\")[:-1])) for \n",
    "                                 fname in self.image_files]))))}\n",
    "        self.xforms = xforms\n",
    "        self.yforms = yforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imf = self.image_files[idx]\n",
    "        bname = path.basename(path.splitext(imf)[0])\n",
    "        ann = path.join(self.ann_dir, bname) + '.png'\n",
    "\n",
    "        # image\n",
    "        image = self.xforms(Image.open(imf)).to(device)\n",
    "\n",
    "        # mask\n",
    "        mask = self.yforms(Image.open(ann)).to(device)\n",
    "\n",
    "        # label\n",
    "        category = path.basename('_'.join(imf.split(\"_\")[:-1]))\n",
    "        labels = torch.tensor([self.breed_assoc[category]]).to(torch.int64).to(device)\n",
    "        \n",
    "        # box\n",
    "        get_edge_pixels = lambda x: ((x* 300 ).floor() - 1) == 2.0\n",
    "        edge_pixels = get_edge_pixels(mask.squeeze())\n",
    "        indices = torch.nonzero(edge_pixels)\n",
    "        \n",
    "        if indices.numel() == 0:\n",
    "            left_x = 0\n",
    "            bottom_y = 0\n",
    "            right_x = 224\n",
    "            top_y = 224\n",
    "\n",
    "        else:\n",
    "            left_x = indices[:,0].min()\n",
    "            right_x = indices[:,0].max()\n",
    "            top_y = indices[:,1].max()\n",
    "            bottom_y = indices[:,1].min()\n",
    "            \n",
    "        boxes = torch.tensor([left_x,bottom_y,right_x,top_y]).unsqueeze(0).to(device)\n",
    "        \n",
    "        return image, {\"boxes\": boxes, \"labels\": labels, \"masks\": mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56b02d-104d-499c-8d5a-154c6a7cf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformx = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "transformy = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "ds = PetDataset(\".\", transformx, transformy)\n",
    "split_ratio = [0.8, 0.2]\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(ds, split_ratio, generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "def custom_collate(batch):\n",
    "    imgs = [b[0] for b in batch]\n",
    "    targets = [b[1] for b in batch]  # Keep targets as a list of dicts\n",
    "    return torch.stack(imgs, dim=0), targets\n",
    "\n",
    "bs = 4\n",
    "dl = DataLoader(train_dataset, shuffle=True, collate_fn=custom_collate, batch_size=bs)\n",
    "tl = DataLoader(test_dataset, collate_fn=custom_collate,batch_size=bs)\n",
    "\n",
    "to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707ddd3-54fb-458d-be43-fd0bc90b3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = det.maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "all_epoch_train_losses = []\n",
    "all_epoch_test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b16e09-ea8e-4c23-951a-1288c99285c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_arr = []\n",
    "    test_loss_arr = []\n",
    "    model.train()\n",
    "    for x, y in tqdm.tqdm(dl, total=len(dl)):\n",
    "        preds = model(x,y)\n",
    "        losses = sum(loss for loss in preds.values())\n",
    "        train_loss_arr.append(losses.item())\n",
    "        opt.zero_grad()\n",
    "        losses.backward()\n",
    "        opt.step()\n",
    "    all_epoch_train_losses.append(train_loss_arr)\n",
    "    print(f'train loss: {sum(loss_arr)/len(loss_arr):0.4f}')\n",
    "    with open(\"train_losses.pkl\", \"rb\") as fd:\n",
    "        pickle.dump(all_epoch_train_losses, fd)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in tqdm.tqdm(tl, total=len(dl)):\n",
    "            preds = model(x,y)\n",
    "            losses = sum(loss for loss in preds.values())\n",
    "            all_epoch_test_losses.append(test_loss_arr)\n",
    "    with open(\"test_losses.pkl\", \"rb\") as fd:\n",
    "        pickle.dump(all_epoch_test_losses, fd)        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cdd25c-3d2e-4bc9-bf10-47bdaead238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(len(loss_arr)), loss_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
