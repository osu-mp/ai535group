{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecaaaa9-56db-4bb1-a754-5e3c7c058303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models.detection as det\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee337ab9-cfee-47fd-bb34-6c701c27e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36cd25-9e5f-4d7f-a016-6bcc9f68eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, root_dir, xforms, yforms):\n",
    "        self.ann_dir = path.join(root_dir, \"annotations\", \"trimaps\")\n",
    "        self.image_dir = path.join(root_dir, \"images\")\n",
    "        self.image_files = glob.glob(path.join(self.image_dir, \"*\"))\n",
    "        self.image_files = [x for x in self.image_files if path.splitext(x)[1] == \".jpg\"]\n",
    "        self.image_files = [x for x in self.image_files if Image.open(x).format == \"JPEG\"]\n",
    "        self.image_files = [x for x in self.image_files if Image.open(x).mode == \"RGB\"]\n",
    "        self.last_mrcnn_idx = 0\n",
    "        self.breed_assoc = {x: self.last_mrcnn_idx + idx for \n",
    "                            idx, x in enumerate(sorted(list(set(\n",
    "                                [ path.basename('_'.join(fname.split(\"_\")[:-1])) for \n",
    "                                 fname in self.image_files]))))}\n",
    "        self.xforms = xforms\n",
    "        self.yforms = yforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imf = self.image_files[idx]\n",
    "        bname = path.basename(path.splitext(imf)[0])\n",
    "        ann = path.join(self.ann_dir, bname) + '.png'\n",
    "\n",
    "        # image\n",
    "        image = self.xforms(Image.open(imf)).to(device)\n",
    "\n",
    "        # mask\n",
    "        mask = self.yforms(Image.open(ann)).to(device)\n",
    "\n",
    "        # label\n",
    "        category = path.basename('_'.join(imf.split(\"_\")[:-1]))\n",
    "        labels = torch.tensor([self.breed_assoc[category]]).to(torch.int64).to(device)\n",
    "        \n",
    "        # box\n",
    "        get_edge_pixels = lambda x: ((x* 300 ).floor() - 1) == 2.0\n",
    "        edge_pixels = get_edge_pixels(mask.squeeze())\n",
    "        indices = torch.nonzero(edge_pixels)\n",
    "        \n",
    "        if indices.numel() == 0:\n",
    "            left_x = 0\n",
    "            bottom_y = 0\n",
    "            right_x = 224\n",
    "            top_y = 224\n",
    "\n",
    "        else:\n",
    "            left_x = indices[:,0].min()\n",
    "            right_x = indices[:,0].max()\n",
    "            top_y = indices[:,1].max()\n",
    "            bottom_y = indices[:,1].min()\n",
    "            \n",
    "        boxes = torch.tensor([left_x,bottom_y,right_x,top_y]).unsqueeze(0).to(device)\n",
    "        \n",
    "        return image, {\"boxes\": boxes, \"labels\": labels, \"masks\": mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56b02d-104d-499c-8d5a-154c6a7cf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformx = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "transformy = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "ds = PetDataset(\".\", transformx, transformy)\n",
    "\n",
    "# debug utils\n",
    "to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ffe6e-aabb-45dc-aa43-06e37caaf0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_im, label = ds[0]\n",
    "# get_edge_pixels = lambda x: ((x* 300 ).floor() - 1) == 2.0\n",
    "# mask = label[\"masks\"].squeeze()\n",
    "# edge_pixels = get_edge_pixels(mask)\n",
    "# indices = torch.nonzero(edge_pixels)\n",
    "# if indices.numel() == 0:\n",
    "#     left_x = 0\n",
    "#     bottom_y = 0\n",
    "#     right_x = 224\n",
    "#     top_y = 224\n",
    "# else:\n",
    "#     left_x = indices[:,0].min()\n",
    "#     right_x = indices[:,0].max()\n",
    "#     top_y = indices[:,1].max()\n",
    "#     bottom_y = indices[:,1].min()\n",
    "# path.splitext(ds.image_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776237b0-1cb4-427b-ab75-33d4d7922d9b",
   "metadata": {},
   "source": [
    "The input to the model is expected to be a list of tensors, each of shape [C, H, W],\n",
    "one for each image, and should be in 0-1 range. Different images can have different sizes.\n",
    "\r\n",
    "The behavior of the model changes depending on if it is in training or evaluation mode.\r\n",
    "\r\n",
    "During training, the model expects both the input tensors and targets (list of dictionary), containn= H.\r\n",
    "\r\n",
    "labels (Int64Tensor[N]): the class label for each ground-tuth box\r\n",
    "\r\n",
    "masks (UInt8Tensor[N, H, W]): the segmentation binary masks for each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa8008-19e8-49ef-922b-cd65e14a4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = det.maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT).to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, (x, y) in tqdm.tqdm(enumerate(ds), total=len(ds)):\n",
    "        preds = model([x],[y])\n",
    "        losses = sum(loss for loss in preds.values())\n",
    "        opt.zero_grad()\n",
    "        losses.backward()\n",
    "        opt.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cdd25c-3d2e-4bc9-bf10-47bdaead238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
